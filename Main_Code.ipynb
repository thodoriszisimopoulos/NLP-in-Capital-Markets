{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69DIWRzxF2Pv"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCZBsbZLeeAW"
      },
      "source": [
        "## Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_o9igHdF-cM"
      },
      "outputs": [],
      "source": [
        "#pip install pandas openpyxl sqlalchemy psycopg2-binary flair langchain langchain-community langchain-openai fuzzywuzzy python-Levenshtein nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV0zHTEDeT23"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfztWIRCeZSA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain.chains import create_sql_query_chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing import List\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIwVDDITIXMc"
      },
      "source": [
        "# SQL Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2P80MFRFLiK"
      },
      "outputs": [],
      "source": [
        "# Set the database URL\n",
        "DATABASE_URL = 'postgresql+psycopg2://thodoris:B4AqjEYBhDPXDHmuSW8MYgfdPp5Nob88@dpg-cpmtb4g8fa8c73aoakig-a.frankfurt-postgres.render.com/capstone_fs'\n",
        "\n",
        "# Create a SQLAlchemy engine\n",
        "engine = create_engine(DATABASE_URL)\n",
        "\n",
        "# Function to load data from the database\n",
        "def load_data_from_db(query):\n",
        "    with engine.connect() as connection:\n",
        "        result = pd.read_sql_query(query, connection)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJL5qPz0JrkG"
      },
      "source": [
        "# Load Excel files in database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQSU5y-VJxSQ"
      },
      "outputs": [],
      "source": [
        "'''# Read the uploaded Excel file into a DataFrame\n",
        "file_name = '/content/Mock up book.xls'  # uploaded to colab this will be deleted\n",
        "df = pd.read_excel(file_name)\n",
        "\n",
        "# Upload the DataFrame to PostgreSQL\n",
        "table_name = 'mock_data'\n",
        "df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
        "\n",
        "print(f\"Uploaded {file_name} to {table_name} table in PostgreSQL.\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOMYbo3hKawX"
      },
      "source": [
        "# Useful functions for querying SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37U0RdSeKuGw"
      },
      "outputs": [],
      "source": [
        "# Lists all tables in database\n",
        "def get_all_tables():\n",
        "    query = \"\"\"\n",
        "    SELECT table_name\n",
        "    FROM information_schema.tables\n",
        "    WHERE table_schema = 'public'\n",
        "    \"\"\"\n",
        "    with engine.connect() as connection:\n",
        "        result = pd.read_sql_query(query, connection)\n",
        "    return result\n",
        "\n",
        "# Lists the columns in a table and their data type\n",
        "def inspect_columns(table_name):\n",
        "    query = f\"\"\"\n",
        "    SELECT column_name, data_type\n",
        "    FROM information_schema.columns\n",
        "    WHERE table_name = '{table_name}'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with engine.connect() as connection:\n",
        "            result = pd.read_sql_query(query, connection)\n",
        "        print(f\"Columns and their data types in the table {table_name}:\\n\")\n",
        "        print(result)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Returns the first 5 rows\n",
        "def get_top_5_rows(table_name):\n",
        "    query = f\"SELECT * FROM {table_name} LIMIT 5\"\n",
        "    with engine.connect() as connection:\n",
        "        result = pd.read_sql_query(query, connection)\n",
        "    return result\n",
        "\n",
        "# Executes a given query\n",
        "def execute_query(table_name, query):\n",
        "    with engine.connect() as connection:\n",
        "        result = pd.read_sql_query(query, connection)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G8wbIhQHZiW"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Example usage\n",
        "inspect_columns('mock_data')\n",
        "\n",
        "table_name = 'mock_data'\n",
        "columns = '\"Feedback\"'\n",
        "query = f\"SELECT {columns} FROM {table_name}\"\n",
        "execute_query(table_name, query)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAkZA2g8VvGr"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12BR3EI6MeHu"
      },
      "outputs": [],
      "source": [
        "table_name = 'mock_data'\n",
        "columns = '\"Feedback\"'\n",
        "query = f\"SELECT {columns} FROM {table_name}\"\n",
        "df = execute_query(table_name, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOfKxSXBY9Ja"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9aWc8fVwrLR"
      },
      "outputs": [],
      "source": [
        "from flair.models import TextClassifier\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "# Import flair Sentence to process input text\n",
        "from flair.data import Sentence\n",
        "# Import accuracy_score to check performance\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "text= df[\"Feedback\"][0]\n",
        "sentence = Sentence(text)\n",
        "classifier.predict(sentence)\n",
        "score = sentence.labels[0].score\n",
        "value = sentence.labels[0].value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd0tPaGdv8WJ"
      },
      "outputs": [],
      "source": [
        "score, value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkMh9efZwAij"
      },
      "outputs": [],
      "source": [
        "df['Feedback'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgXa_SpFxx7g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un9EV4FplLmb"
      },
      "source": [
        "# Text 2 SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12tfFIDnoZak"
      },
      "source": [
        "## Approach 1 (Working)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bEok3_6Vnuw"
      },
      "source": [
        "Step 1: Connect to the Database and Obtain Table and Column Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy5TlqqhodnS"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import create_engine, MetaData\n",
        "\n",
        "# Database URL\n",
        "DATABASE_URL = 'postgresql+psycopg2://thodoris:B4AqjEYBhDPXDHmuSW8MYgfdPp5Nob88@dpg-cpmtb4g8fa8c73aoakig-a.frankfurt-postgres.render.com/capstone_fs'\n",
        "\n",
        "# Create an engine\n",
        "engine = create_engine(DATABASE_URL)\n",
        "\n",
        "# Connect to the database and fetch table and column names\n",
        "metadata = MetaData()\n",
        "metadata.reflect(bind=engine)\n",
        "\n",
        "# Store table and column names\n",
        "table_columns = {}\n",
        "for table in metadata.tables.values():\n",
        "    table_columns[table.name] = [column.name for column in table.c]\n",
        "\n",
        "# Print table and column names\n",
        "for table, columns in table_columns.items():\n",
        "    print(f\"Table: {table}, Columns: {columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qon6z3JKVp5I"
      },
      "source": [
        "Step 2: Use NLP to Parse the User Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNKkWfMvSjMP"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from fuzzywuzzy import process\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load a pre-trained model for zero-shot classification\n",
        "nlp_model = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Example categories for intent classification\n",
        "categories = [\"count\", \"retrieve\", \"average\", \"sum\", \"min\", \"max\", \"unique count\"]\n",
        "\n",
        "# Filter-indicating words\n",
        "filter_keywords = {'that', 'where', 'which', 'top', 'first', 'last', 'over', 'under', 'greater', 'less'}\n",
        "\n",
        "def parse_user_query():\n",
        "    question = input(\"Please enter your question: \")\n",
        "    intent_result = nlp_model(question, candidate_labels=categories)\n",
        "    intent = intent_result['labels'][0]  # Most likely intent\n",
        "    return intent, question\n",
        "\n",
        "def extract_pre_filter_keywords(question):\n",
        "    # Split question at the filter keywords\n",
        "    tokens = word_tokenize(question.lower())\n",
        "    split_index = len(tokens)\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token in filter_keywords:\n",
        "            split_index = i\n",
        "            break\n",
        "\n",
        "    pre_filter_tokens = tokens[:split_index]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    keywords = [word for word in pre_filter_tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    print(f\"Extracted pre-filter keywords: {keywords}\")  # Debugging\n",
        "    return keywords\n",
        "\n",
        "def match_columns(keywords, priority_terms=[]):\n",
        "    matched_columns = []\n",
        "    for table, col_list in table_columns.items():\n",
        "        for keyword in keywords:\n",
        "            closest_match = process.extract(keyword, col_list, limit=3)\n",
        "            for match in closest_match:\n",
        "                matched_columns.append((table, match[0], match[1]))\n",
        "\n",
        "    # Increase confidence for priority terms\n",
        "    for i, (table, column, confidence) in enumerate(matched_columns):\n",
        "        for term in priority_terms:\n",
        "            if term in column.lower():\n",
        "                matched_columns[i] = (table, column, confidence + 20)\n",
        "\n",
        "    matched_columns = sorted(matched_columns, key=lambda x: x[2], reverse=True)  # Sort by confidence, descending\n",
        "    print(f\"Matched columns: {matched_columns}\")  # Debugging\n",
        "    return matched_columns\n",
        "\n",
        "def needs_filter(question):\n",
        "    tokens = word_tokenize(question.lower())\n",
        "    return any(token in filter_keywords for token in tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV-F6GdbV4GD"
      },
      "source": [
        "Step 3: Dynamically Form and Execute SQL Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bngFL5I5SjOp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy.sql import text\n",
        "\n",
        "def extract_filter_conditions(filter_input):\n",
        "    tokens = word_tokenize(filter_input.lower())\n",
        "    conditions = []\n",
        "    current_column = None\n",
        "    operator = None\n",
        "\n",
        "    # Define possible operators and their NLP equivalents\n",
        "    operators = {\n",
        "        'greater': '>',\n",
        "        'more': '>',\n",
        "        'less': '<',\n",
        "        'fewer': '<',\n",
        "        'equal': '=',\n",
        "        'over': '>',\n",
        "        'under': '<',\n",
        "        'above': '>',\n",
        "        'below': '<'\n",
        "    }\n",
        "\n",
        "    for token in tokens:\n",
        "        if token.isalpha() and token.lower() not in operators:\n",
        "            current_column = token\n",
        "        elif token in ['>', '<', '>=', '<=', '=', '!=']:\n",
        "            operator = token\n",
        "        elif token.lower() in operators:\n",
        "            operator = operators[token.lower()]\n",
        "        elif token.isdigit() or (token.replace('.', '', 1).isdigit() and token.count('.') < 2):\n",
        "            if current_column and operator:\n",
        "                conditions.append((current_column, operator, token))\n",
        "                current_column = None\n",
        "                operator = None\n",
        "\n",
        "    print(f\"Extracted filter conditions: {conditions}\")  # Debugging\n",
        "    return conditions\n",
        "\n",
        "def form_query(parsed_intent, question):\n",
        "    pre_filter_keywords = extract_pre_filter_keywords(question)\n",
        "    matched_columns = match_columns(pre_filter_keywords, priority_terms=[\"investor\", \"firm\"])\n",
        "\n",
        "    if not matched_columns:\n",
        "        print(\"No columns matched.\")\n",
        "        return None\n",
        "\n",
        "    select_columns = []\n",
        "    for table, column, confidence in matched_columns:\n",
        "        if confidence >= 95:\n",
        "            select_columns.append((table, column))\n",
        "        else:\n",
        "            break  # Only show suggestions if confidence is below 95%\n",
        "\n",
        "    if not select_columns and matched_columns:\n",
        "        print(f\"\\nThe script is not fully confident about the columns. Here are the suggestions:\")\n",
        "        for i, (table, column, confidence) in enumerate(matched_columns):\n",
        "            print(f\"{i+1}. Table: {table}, Column: {column}, Confidence: {confidence}%\")\n",
        "\n",
        "        choice = input(\"Choose the number of the correct column or press enter to use the highest confidence match: \")\n",
        "        if choice.isdigit() and 1 <= int(choice) <= len(matched_columns):\n",
        "            index = int(choice) - 1\n",
        "            select_columns.append((matched_columns[index][0], matched_columns[index][1]))\n",
        "        else:\n",
        "            select_columns.append((matched_columns[0][0], matched_columns[0][1]))  # Use highest confidence match\n",
        "\n",
        "    if not select_columns:\n",
        "        print(\"No columns confirmed.\")\n",
        "        return None\n",
        "\n",
        "    where_clause = ''\n",
        "    if needs_filter(question):\n",
        "        filter_input = input(\"As I understand, you want to filter some of your results. Explain to me in your words the filter that should be applied. If you do not want an extra filter, type 'Continue': \")\n",
        "        if filter_input.lower() != 'continue':\n",
        "            filter_conditions = extract_filter_conditions(filter_input)\n",
        "            for col_keyword, operator, value in filter_conditions:\n",
        "                matched_filter_columns = match_columns([col_keyword])\n",
        "                if matched_filter_columns:\n",
        "                    best_match = matched_filter_columns[0][1]\n",
        "                    where_clause += f' AND \"{best_match}\" {operator} {value}'\n",
        "            if where_clause:\n",
        "                where_clause = where_clause.lstrip(' AND')\n",
        "\n",
        "    query = None\n",
        "    if parsed_intent == \"count\":\n",
        "        for table, column in select_columns:\n",
        "            query = f'SELECT COUNT(\"{column}\") FROM {table}'\n",
        "            if where_clause:\n",
        "                query += f' WHERE {where_clause}'\n",
        "            return query\n",
        "    elif parsed_intent == \"retrieve\":\n",
        "        for table, column in select_columns:\n",
        "            query = f'SELECT * FROM {table}'\n",
        "            if where_clause:\n",
        "                query += f' WHERE {where_clause}'\n",
        "            return query\n",
        "    elif parsed_intent == \"average\":\n",
        "        for table, column in select_columns:\n",
        "            query = f'SELECT AVG(\"{column}\") FROM {table}'\n",
        "            if where_clause:\n",
        "                query += f' WHERE {where_clause}'\n",
        "            return query\n",
        "    elif parsed_intent == \"sum\":\n",
        "        for table, column in select_columns:\n",
        "            query = f'SELECT SUM(\"{column}\") FROM {table}'\n",
        "            if where_clause:\n",
        "                query += f' WHERE {where_clause}'\n",
        "            return query\n",
        "    elif parsed_intent == \"min\":\n",
        "        for table, column in select_columns:\n",
        "            query = f'SELECT MIN(\"{column}\") FROM {table}'\n",
        "            if where_clause:\n",
        "                query += f' WHERE {where_clause}'\n",
        "            return query\n",
        "    elif parsed_intent == \"max\":\n",
        "        for table, column in select_columns:\n",
        "            query = f'SELECT MAX(\"{column}\") FROM {table}'\n",
        "            if where_clause:\n",
        "                query += f' WHERE {where_clause}'\n",
        "            return query\n",
        "    elif parsed_intent == \"unique count\":\n",
        "        for table, column in select_columns:\n",
        "            query = f'SELECT COUNT(DISTINCT \"{column}\") FROM {table}'\n",
        "            if where_clause:\n",
        "                query += f' WHERE {where_clause}'\n",
        "            return query\n",
        "    return None\n",
        "\n",
        "def execute_query(query):\n",
        "    with engine.connect() as connection:\n",
        "        print(f\"Executing query: {query}\")  # Debug print\n",
        "        result = connection.execute(text(query))\n",
        "        df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um5R3KcHV8H-"
      },
      "source": [
        "Step 4: Explain Results in Natural Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAMXYxb0SjR_"
      },
      "outputs": [],
      "source": [
        "def explain_results(result_df, parsed_intent, original_question):\n",
        "    if result_df.empty:\n",
        "        return \"No data found.\"\n",
        "\n",
        "    if parsed_intent == \"count\":\n",
        "        count = result_df.iloc[0, 0]\n",
        "        explanation = f\"There are {count} records matching your query.\"\n",
        "    elif parsed_intent == \"retrieve\":\n",
        "        explanation = f\"The query returned {len(result_df)} records. Here are the details:\\n{result_df.to_string(index=False)}\"\n",
        "    elif parsed_intent == \"average\":\n",
        "        avg_value = result_df.iloc[0, 0]\n",
        "        explanation = f\"The average value is {avg_value}.\"\n",
        "    elif parsed_intent == \"sum\":\n",
        "        sum_value = result_df.iloc[0, 0]\n",
        "        explanation = f\"The total sum is {sum_value}.\"\n",
        "    elif parsed_intent == \"min\":\n",
        "        min_value = result_df.iloc[0, 0]\n",
        "        explanation = f\"The minimum value is {min_value}.\"\n",
        "    elif parsed_intent == \"max\":\n",
        "        max_value = result_df.iloc[0, 0]\n",
        "        explanation = f\"The maximum value is {max_value}.\"\n",
        "    elif parsed_intent == \"unique count\":\n",
        "        unique_count = result_df.iloc[0, 0]\n",
        "        explanation = f\"There are {unique_count} unique records matching your query.\"\n",
        "    else:\n",
        "        explanation = \"Query executed successfully.\"\n",
        "\n",
        "    return explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roR6E1K-ZIGX"
      },
      "outputs": [],
      "source": [
        "# Main script\n",
        "parsed_intent, question = parse_user_query()\n",
        "query = form_query(parsed_intent, question)\n",
        "if query:\n",
        "    result_df = execute_query(query)\n",
        "    explanation = explain_results(result_df, parsed_intent, question)\n",
        "    output_message = f\"The intent of the question was: {parsed_intent}\\n\\nThe question was: {question}\\n\\n{explanation}\"\n",
        "    print(output_message)\n",
        "else:\n",
        "    print(\"Could not generate a query based on the input.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxy3vnIjnGx8"
      },
      "source": [
        "## Approach 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWfjMBAaoXQO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "giz9eRRgnMm1"
      },
      "outputs": [],
      "source": [
        "%pip install transformers sqlalchemy psycopg2 pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "62lrsDZ9ojgz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sqlalchemy import create_engine\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZY8sxJKowCR"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"suriya7/t5-base-text-to-sql\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"suriya7/t5-base-text-to-sql\")\n",
        "\n",
        "def translate_to_sql_select(english_query):\n",
        "    input_text = f\"translate English to SQL: {english_query}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
        "    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return sql_query\n",
        "\n",
        "# Set the database URL\n",
        "DATABASE_URL = 'postgresql+psycopg2://thodoris:B4AqjEYBhDPXDHmuSW8MYgfdPp5Nob88@dpg-cpmtb4g8fa8c73aoakig-a.frankfurt-postgres.render.com/capstone_fs'\n",
        "\n",
        "# Create a SQLAlchemy engine\n",
        "engine = create_engine(DATABASE_URL)\n",
        "\n",
        "# Function to load data from the database\n",
        "def load_data_from_db(query):\n",
        "    with engine.connect() as connection:\n",
        "        result = pd.read_sql_query(query, connection)\n",
        "    return result\n",
        "\n",
        "# Function to upload Excel data to PostgreSQL\n",
        "def upload_excel_to_db(file_name, table_name, engine):\n",
        "    # Read the uploaded Excel file into a DataFrame\n",
        "    df = pd.read_excel(file_name)\n",
        "    # Upload the DataFrame to PostgreSQL\n",
        "    df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
        "    print(f\"Uploaded {file_name} to {table_name} table in PostgreSQL.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9oWEAGBpcBT"
      },
      "outputs": [],
      "source": [
        "# File name and table name\n",
        "file_name = '/content/Mock up book.xls'  # Change the path as necessary\n",
        "table_name = 'mock_data'\n",
        "\n",
        "# Upload the Excel data to the database\n",
        "upload_excel_to_db(file_name, table_name, engine)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-0I7kCXt2DV"
      },
      "outputs": [],
      "source": [
        "table_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUZXidClt0x1"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "english_query = \"Show one investor id with firm alloc greater than 5000\"\n",
        "sql_query = translate_to_sql_select(english_query)\n",
        "print(\"SQL Query:\", sql_query)\n",
        "\n",
        "# Load data from the database\n",
        "try:\n",
        "    data = load_data_from_db(sql_query)\n",
        "    print(\"Data from the database:\")\n",
        "    print(data)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5bx_UBjv35r"
      },
      "source": [
        "## Approach 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCCs-zx-wfAL"
      },
      "outputs": [],
      "source": [
        "%pip install transformers torch sqlalchemy pandas psycopg2-binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BjryHhlv7rc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, inspect\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Set the database URL\n",
        "DATABASE_URL = 'postgresql+psycopg2://thodoris:B4AqjEYBhDPXDHmuSW8MYgfdPp5Nob88@dpg-cpmtb4g8fa8c73aoakig-a.frankfurt-postgres.render.com/capstone_fs'\n",
        "\n",
        "# Create a SQLAlchemy engine\n",
        "engine = create_engine(DATABASE_URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfnrshXnxIeJ"
      },
      "outputs": [],
      "source": [
        "# Function to read all tables and columns from the database\n",
        "def get_db_schema(engine):\n",
        "    inspector = inspect(engine)\n",
        "    schema = {}\n",
        "    for table_name in inspector.get_table_names():\n",
        "        columns = inspector.get_columns(table_name)\n",
        "        schema[table_name] = [column['name'] for column in columns]\n",
        "    return schema\n",
        "\n",
        "# Format schema for better input to the model\n",
        "def format_schema(schema):\n",
        "    formatted_schema = []\n",
        "    for table, columns in schema.items():\n",
        "        formatted_columns = \", \".join(columns)\n",
        "        formatted_schema.append(f\"{table}({formatted_columns})\")\n",
        "    return \" | \".join(formatted_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y9LiKktxq7T"
      },
      "outputs": [],
      "source": [
        "# Load the schema\n",
        "schema = get_db_schema(engine)\n",
        "formatted_schema = format_schema(schema)\n",
        "print(formatted_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPOQeW39xrDK"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('cssupport/t5-small-awesome-text-to-sql')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXwhiY6-xrKE"
      },
      "outputs": [],
      "source": [
        "# Function to generate SQL query from natural language prompt\n",
        "def generate_sql(prompt, formatted_schema):\n",
        "    input_text = f\"{prompt} | {formatted_schema}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(device)\n",
        "    outputs = model.generate(**inputs, max_length=512)\n",
        "    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return sql_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-X8VgWmxrM4"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "prompt = \"Show one investor id with firm alloc greater than 5000\"\n",
        "sql_query = generate_sql(prompt, formatted_schema)\n",
        "print(f\"Generated SQL query: {sql_query}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7iYPEUsxrPy"
      },
      "outputs": [],
      "source": [
        "# Function to load data from the database\n",
        "def load_data_from_db(query):\n",
        "    with engine.connect() as connection:\n",
        "        result = pd.read_sql_query(query, connection)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiEYiThrxrSL"
      },
      "outputs": [],
      "source": [
        "# Execute the query\n",
        "result_df = load_data_from_db(sql_query)\n",
        "print(result_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsGGicgrxrYl"
      },
      "outputs": [],
      "source": [
        "# Function to explain the results\n",
        "def explain_results(result_df):\n",
        "    num_investors = result_df.iloc[0, 0] if not result_df.empty else 0\n",
        "    return f\"There are {num_investors} investors.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_JO483zx5Om"
      },
      "outputs": [],
      "source": [
        "# Explain the results\n",
        "explanation = explain_results(result_df)\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfnbc-ehzSDw"
      },
      "source": [
        "## Approach 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmntt21gzVFT"
      },
      "outputs": [],
      "source": [
        "%pip install transformers torch sqlalchemy pandas psycopg2-binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4sOQvonzWIS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, inspect\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Set the database URL\n",
        "DATABASE_URL = 'postgresql+psycopg2://thodoris:B4AqjEYBhDPXDHmuSW8MYgfdPp5Nob88@dpg-cpmtb4g8fa8c73aoakig-a.frankfurt-postgres.render.com/capstone_fs'\n",
        "\n",
        "# Create a SQLAlchemy engine\n",
        "engine = create_engine(DATABASE_URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3amQM1JziqB"
      },
      "outputs": [],
      "source": [
        "# Function to read all tables and columns from the database\n",
        "def get_db_schema(engine):\n",
        "    inspector = inspect(engine)\n",
        "    schema = {}\n",
        "    for table_name in inspector.get_table_names():\n",
        "        columns = inspector.get_columns(table_name)\n",
        "        schema[table_name] = [column['name'] for column in columns]\n",
        "    return schema\n",
        "\n",
        "# Format schema for better input to the model\n",
        "def format_schema(schema):\n",
        "    formatted_schema = []\n",
        "    for table, columns in schema.items():\n",
        "        formatted_columns = \", \".join(columns)\n",
        "        formatted_schema.append(f\"{table}({formatted_columns})\")\n",
        "    return \" | \".join(formatted_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCY9h_Qfzis0"
      },
      "outputs": [],
      "source": [
        "# Load the schema\n",
        "schema = get_db_schema(engine)\n",
        "formatted_schema = format_schema(schema)\n",
        "print(formatted_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzTK61qwziu7"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"suriya7/t5-base-text-to-sql\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"suriya7/t5-base-text-to-sql\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIzU3zH3zi0y"
      },
      "outputs": [],
      "source": [
        "# Function to generate SQL query from natural language prompt\n",
        "def translate_to_sql_select(english_query):\n",
        "    input_text = f\"translate English to SQL: {english_query}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
        "    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return sql_query\n",
        "\n",
        "# Postprocess SQL query to correct common errors\n",
        "def postprocess_sql(sql_query):\n",
        "    sql_query = sql_query.replace(' , ', ', ')\n",
        "    sql_query = sql_query.replace(' .', '.')\n",
        "    if 'SELECT' not in sql_query.upper():\n",
        "        sql_query = 'SELECT ' + sql_query\n",
        "    return sql_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgsbFtqCzi7o"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "prompt = \"How many investors ids are there\"\n",
        "sql_query = translate_to_sql_select(prompt)\n",
        "sql_query = postprocess_sql(sql_query)\n",
        "print(f\"Generated SQL query: {sql_query}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3xuKqM4zssx"
      },
      "outputs": [],
      "source": [
        "# Function to load data from the database\n",
        "def load_data_from_db(query):\n",
        "    with engine.connect() as connection:\n",
        "        result = pd.read_sql_query(query, connection)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToM0vCiuzsvY"
      },
      "outputs": [],
      "source": [
        "# Execute the query\n",
        "result_df = load_data_from_db(sql_query)\n",
        "print(result_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqr_HpezzsyC"
      },
      "outputs": [],
      "source": [
        "# Function to explain the results\n",
        "def explain_results(result_df):\n",
        "    if result_df.empty:\n",
        "        return \"There are no investors available.\"\n",
        "    num_investors = result_df.iloc[0, 0] if len(result_df.columns) == 1 else len(result_df)\n",
        "    return f\"There are {num_investors} investors.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otbMa7cBzwW5"
      },
      "outputs": [],
      "source": [
        "# Explain the results\n",
        "explanation = explain_results(result_df)\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g0PQIkb3CaT"
      },
      "source": [
        "## Approach 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zkU3VQK3Eiw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, inspect\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Set the database URL\n",
        "DATABASE_URL = 'postgresql+psycopg2://thodoris:B4AqjEYBhDPXDHmuSW8MYgfdPp5Nob88@dpg-cpmtb4g8fa8c73aoakig-a.frankfurt-postgres.render.com/capstone_fs'\n",
        "\n",
        "# Create a SQLAlchemy engine\n",
        "engine = create_engine(DATABASE_URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7ScziSJ4CY9"
      },
      "outputs": [],
      "source": [
        "# Function to read all tables and columns from the database\n",
        "def get_db_schema(engine):\n",
        "    inspector = inspect(engine)\n",
        "    schema = {}\n",
        "    for table_name in inspector.get_table_names():\n",
        "        columns = inspector.get_columns(table_name)\n",
        "        schema[table_name] = [column['name'] for column in columns]\n",
        "    return schema\n",
        "\n",
        "# Format schema for better input to the model\n",
        "def format_schema(schema):\n",
        "    formatted_schema = []\n",
        "    for table, columns in schema.items():\n",
        "        formatted_columns = \", \".join(columns)\n",
        "        formatted_schema.append(f\"{table}({formatted_columns})\")\n",
        "    return \" | \".join(formatted_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Itik8Cwi4G4x"
      },
      "outputs": [],
      "source": [
        "# Load the schema\n",
        "schema = get_db_schema(engine)\n",
        "formatted_schema = format_schema(schema)\n",
        "print(\"Formatted Schema:\", formatted_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbu_mjx74G7a"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"suriya7/t5-base-text-to-sql\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"suriya7/t5-base-text-to-sql\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it84Vv3w4G9r"
      },
      "outputs": [],
      "source": [
        "# Function to generate SQL query from natural language prompt\n",
        "def translate_to_sql_select(english_query, formatted_schema):\n",
        "    input_text = f\"Query: {english_query}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
        "    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return sql_query\n",
        "\n",
        "# Postprocess SQL query to correct common errors\n",
        "def postprocess_sql(sql_query):\n",
        "    sql_query = sql_query.replace(' , ', ', ')\n",
        "    sql_query = sql_query.replace(' .', '.')\n",
        "    if 'SELECT' not in sql_query.upper():\n",
        "        sql_query = 'SELECT ' + sql_query\n",
        "    return sql_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxeYZP_E4G_3"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "prompt = \"How many investors are there?\"\n",
        "sql_query = translate_to_sql_select(prompt, formatted_schema)\n",
        "sql_query = postprocess_sql(sql_query)\n",
        "print(f\"Generated SQL query: {sql_query}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApYUBAY14HCa"
      },
      "outputs": [],
      "source": [
        "# Function to load data from the database\n",
        "def load_data_from_db(query):\n",
        "    with engine.connect() as connection:\n",
        "        result = pd.read_sql_query(query, connection)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTJtnm9k4HEv"
      },
      "outputs": [],
      "source": [
        "# Execute the query\n",
        "try:\n",
        "    result_df = load_data_from_db(sql_query)\n",
        "    print(result_df)\n",
        "except Exception as e:\n",
        "    print(f\"Error executing query: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW0gRQPw4HJT"
      },
      "outputs": [],
      "source": [
        "# Function to explain the results\n",
        "def explain_results(result_df):\n",
        "    if result_df.empty:\n",
        "        return \"There are no investors available.\"\n",
        "    num_investors = result_df.iloc[0, 0] if len(result_df.columns) == 1 else len(result_df)\n",
        "    return f\"There are {num_investors} investors.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uht3UMiy4b5j"
      },
      "outputs": [],
      "source": [
        "# Explain the results\n",
        "explanation = explain_results(result_df)\n",
        "print(explanation)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "69DIWRzxF2Pv",
        "QIwVDDITIXMc",
        "hJL5qPz0JrkG",
        "rOMYbo3hKawX",
        "MAkZA2g8VvGr",
        "hxy3vnIjnGx8",
        "m5bx_UBjv35r",
        "vfnbc-ehzSDw",
        "8g0PQIkb3CaT"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}